# Least-squares with constraints

In many applications, optimisation problems are posed with constraints. We consider here constrained least-squares problems of the form

$$
\begin{array}{ll}
\min_x & \| A x - a \|_2 \\
\text{subject to} &\\
& C x = c,\\
& D x \ge d.
\end{array}
$$

where $a \in \mathbb{R}^n$, $c \in \mathbb{R}^m$, $d \in \mathbb{R}^p$, $C \in \mathbb{R}^{m \times n}$, $D \in \mathbb{R}^{p \times n}$ and $A \in \mathbb{R}^{n \times n}$. The expression $D x \ge d$ is to be understood componentwise: for $d, e \in \mathbb{R}^p$,

$$
d \ge e \qquad \Leftrightarrow \qquad d_i \ge e_i \quad \forall \; i \in \{1, \ldots, p\}.
$$

Because of the constraints, the techniques from the previous section do not apply directly, as the problem is inherently nonlinear.

Setting $B = A^\top A$ and $b = - A^\top a$, we rewrite the above least-squares problem in the generic form of a quadratic programme.

```{admonition} Primal problem (quadratic programme)
$$
\begin{array}{ll}
\min_x & \frac{1}{2} x^\top B x + b^\top x\\
\text{subject to} &\\
& C x - c = 0,\\
& D x - d \ge 0.
\end{array}
$$
```

Both problems have the same minimisers because

$$
\textstyle \frac{1}{2} \| A x - a \|_2^2 = \frac{1}{2} x^\top A^\top A  x - a^\top A x + \frac{1}{2} a^\top a = x^\top B x + b^\top x + \frac{1}{2} a^\top a,
$$

noting that the constant term $\frac{1}{2} a^\top a$ can be removed without changing the set of minimisers. Because of our initial assumption $B = A^\top A$ we shall assume throughout the section that $B$ is symmetric and positive semi-definite. We refer to $\frac{1}{2} x^\top B x + b^\top x$ as the *objective function*, $C x - c = 0$ the *equality constraint* and $D x - d \ge 0$ the *inequality constraint*. The *value* of the optimal control problem is that value of the objective function when evaluated at an optimal solution.

Observe, that the value of the initial problem in least-squares form differs from that of quadratic programme because of the factor $\frac{1}{2}$ and the squaring of the norm and the constant term, even if the set of optimal solutions coincide. However, given a optimal solution $x$ it is simple to compute the value $\| A x - a \|_2$.

## Certificates of optimality

In this section, we learn about *certificates of optimality*, which address a fundamental point: The algorithms to solve constrained least-squares problem are outside of the scope of this module. How, then, can you solve such problems with confidence? More generally, how can you use numerical methods, that you have not mathematically fully understood? 

This is a question you face now, as you only begin with computational mathematics and want to use a wide range of methods as quickly as possible. But even as a seasoned numerical analyst, you will need to think about deploying mathematical algorithms in application scenarios where users are not domain experts.

A key realisation is that verifying whether a candidate solution is indeed equal to (or at least a good approximation of) an exaxt solution is often much simpler than finding such a candidate in the first place.

You are familiar with this situation from other areas of mathematics: For

$$
F(t) = \int_0^\top f(x) dx,
$$

it can be hard to find $F$ given $f$, but once a candidate $F$ is found, it is relatively easy to check that its derivative is $f$. Similarly, various encryption methods rely on the fact that it is easier to multiply two prime numbers than to find the prime factors of a given product.

A certificate of optimality consists of a candidate solution together with additional information that makes it relatively easy to confirm the quality of the candidate solution; even if the method by which the candidate solution was obtained is not examined. In other areas of numerical analysis and scientific computing one finds similar concepts; for example, *a posteriori error bounds* are used to appraise the quality of numerical solutions of differential equations.

## Duality

There are different types of certificates of optimality; many of them are based on the important concept of *duality*, which plays a key role throughout scientific computing; differential equations again serve as a good example.

The *Lagrangian function* associated with the primal problem is

$$
L(x, y, s) := \frac{1}{2} x^\top B x + b^\top x - y^\top (C x - c) - s^\top (D x - d).
$$

The variables $y$ and $s$ are called *Lagrangian multipliers*. All information about the primal problem is encoded within the Lagrangian function through the observation that for any given vector $x$ (self-check question)

$$
\max_{y, s \atop s \geq 0} L(x, y, s) =
\begin{cases}
\frac{1}{2} x^\top B x + b^\top x & \text{if $C x = c$ and $D x \geq d$,}\\
+\infty & \text{otherwise.}
\end{cases}
$$

Therefore, the primal problem can be recast in a more abstract form.

```{admonition} Primal problem (abstract form)
$$
\min_x \max_{y, s \atop s \geq 0} L(x, y, s)
$$
```

The *dual problem* is obtained by reversing the order of the min and max operations.

```{admonition} Dual problem (abstract form)
$$
\max_{y, s \atop s \geq 0} \min_x L(x, y, s)
$$
```

Within mathematics, we speak of duality when we identify two structures $A$ and $B$ and a transformation $\Phi$, where $\Phi(A) = B$ and $\Phi(B) = A$. Here $\Phi$ corresponds to the reversal of $\max$ and $\min$. Now convince yourself (self-check question) that the dual problem can be written as follows:

```{admonition} Dual problem (quadratic programme)
$$
\begin{array}{ll}
\max_{x,y,s} & c^\top y + d^\top s - \frac{1}{2} x^\top B x\\
\text{subject to} &\\
& C^\top y + D^\top s - B x = b,\\
& s \ge 0.
\end{array}
$$
```

We call $x$ a *feasible point* of the primal problem if $C x = c$ and $D x \ge d$. Similarly, $(x,y,s)$ is feasible for the dual problem if $C^\top y + D^\top s - B x = b$ and $s \ge 0$. In short, feasible points are those that satisfy the constraints.

There is a fundamental connection between the primal and dual problem

```{prf:theorem} Weak duality
Assume that $\bar{x}$ and $(x,y,s)$ are feasible points of the primal and dual problems, respectively. Then

$$
c^\top y + d^\top s - \frac{1}{2} x^\top B x \leq \frac{1}{2} \bar{x}^\top B \bar{x} + b^\top \bar{x}.
$$

In other words, among feasible points, the objective function of the dual problem attains values below than or equal to those of the primal function.
```

```{prf:proof}
For feasible $x$ and $(x,y,s)$,

$$
\begin{align*}
c^\top y + d^\top s - \frac{1}{2} x^\top B x \, \leq \, & (C \bar{x})^\top y + (D \bar{x})^\top s - \frac{1}{2} x^\top B x\\
= \, & (C^\top y + D^\top s)^\top \bar{x} - \frac{1}{2} x^\top B x\\
= \, & (b + B x)^\top \bar{x} - \frac{1}{2} x^\top B x\\
= \, & \frac{1}{2} \bar{x}^\top B \bar{x} + b^\top \bar{x} - \frac{1}{2} (x - \bar{x})^\top B (x - \bar{x})\\
\leq \, & \frac{1}{2} \bar{x}^\top B \bar{x} + b^\top \bar{x}.
\end{align*}
$$
```

Bearing the weak duality theorem in mind, we call a pair $(\bar{x}, (x,y,s))$ a certificate of optimality. To check that $\bar{x}$ is indeed solution to the primal problem we need to do the following:

1. Check that $\bar{x}$ is a feasible point of the primal problem.
2. Check that $(x,y,s)$ is a feasible point of the dual problem.
3. Check that $c^\top y + d^\top s - \frac{1}{2} x^\top B x = \frac{1}{2} \bar{x}^\top B \bar{x} + b^\top \bar{x}$.

All those steps only require the evaluation of simple matrix vector and scalar products.

Why does this work? Suppose that $\bar{x}$ and $(x,y,s)$ passed the above tests 1., 2. and 3., but $\bar{x}$ was not an optimal solution. Then there exists another feasible point $\tilde{x}$ of the primal problem with an even lower value:

$$
\frac{1}{2} \tilde{x}^\top B \tilde{x} + b^\top \tilde{x} < \frac{1}{2} \bar{x}^\top B \bar{x} + b^\top \bar{x} = c^\top y + d^\top s - \frac{1}{2} x^\top B x.
$$

This is impossible by weak duality, and therefore $\bar{x}$ must be optimal.

With $\bar{x}$ and $(x,y,s)$ optimal, the difference

$$
\underbrace{\frac{1}{2} \bar{x}^\top B \bar{x} + b^\top \bar{x}}_{\text{optimal value of the primal problem}} - \underbrace{c^\top y + d^\top s - \frac{1}{2} x^\top B x}_{\text{optimal value of the dual problem}}
$$

is called the *duality gap*.

One might wonder if certificates of the type $(\bar{x}, (x,y,s))$ are too restrictive: Could there be optimisation problems for which the duality gap exceeds 0? Then the above certificate of optimality would not work even if optimal solutions are found (self-check question). The following deep result, known as strong duality, shows that this cannot happen for constrained least-squares problems.

```{prf:theorem} Strong duality
Assume that either the primal or dual problem has a feasible point. Then this problem is bounded if and only if the other has a feasible point.  In such cases, both problems have optimal solutions and share the same optimal value.
```

```{prf:proof}
The proof is beyond the scope of the lecture notes. See {cite}`NocedalWright` for details.
```

In contrast, many optimisation problems, for example with non-convex objective function or constraints, exhibit a positive duality gap, making it more difficult to certify optimality.

## Python skills

[CVXPY](https://www.cvxpy.org/) is a Python library designed to make the construction and solving of quadratic and other convex optimisation problems straightforward. Its high-level syntax allows users to specify an objective and constraints in a manner similar to writing down the mathematical formulation. Under the hood, CVXPY automatically transforms the problem into a canonical form, selects an appropriate solver, and returns primal and dual solutions, all while handling domain restrictions, dual variables, and convergence tolerances. This streamlined workflow makes it an ideal tool for both [instructional examples](https://www.cvxpy.org/examples/index.html) and real-world applications.

### Least-squares problems

The code below illustrates a template for solving least-squares problems of the type discussed above. Here, `A`, `a`, `C`, `c`, `D` and `d` are initialised randomly for demonstration. By substituting specific data, one can easily solve practical least-squares problems.

```python
import cvxpy as cp
import numpy as np

# ------------------------------------------------
# Example data (adjust as appropriate).
# ------------------------------------------------
n, m, p = 5, 2, 3
np.random.seed(0)

# Construct a least-squares setup:
A = np.random.randn(n, n)
a = np.random.randn(n)
B = A.T @ A
b_vec = -A.T @ a

# Constraints:
#   Cx - c = 0  ->  in CVXPY: C@x == c
#   Dx - d >= 0 ->  in CVXPY: D@x >= d
C = np.random.randn(m, n)
c = np.random.randn(m)
D = np.random.randn(p, n)
d = np.random.randn(p)

# Define the variable and constraints in CVXPY
x = cp.Variable(n)
constraints = [
    C @ x == c,  # equality
    D @ x >= d   # inequality
]

# Objective
objective = 0.5 * cp.quad_form(x, B) + b_vec @ x

# Formulate the problem
problem = cp.Problem(cp.Minimize(objective), constraints)

# Solve
primal_value = problem.solve(solver=cp.SCS)  # or another solver
print("Primal objective value:", primal_value)
print("Primal solution x:", x.value)

# Original least-squares problem
ls_value = np.linalg.norm(A @ x.value - a, 2)
print("Original least-squares objective:", ls_value)
```

This code returns the optimiser `x`, the value of the primal quadratic programme, and the value of the original least-squares problem. To check that CVXPY has indeed found an optimal solution, we also retrieve the dual variables.

```python
# Obtain y and s from the solver.
# CVXPY stores in constraints[0].dual_value the variable -y instead of y
y = -constraints[0].dual_value  
s = constraints[1].dual_value
print("Dual variable y (equality):", y)
print("Dual variable s (inequality):", s)

# Compute x from the dual problem:
rhs = (C.T @ y) + (D.T @ s) - b_vec
xDual = np.linalg.pinv(B) @ rhs  # Use the pseudo-inverse if B is semidefinite.

# Evaluate the dual objective
dual_value = (c @ y) + (d @ s) - 0.5*(xDual.T @ B @ xDual)
print("Dual objective value:", dual_value)

# Duality gap
gap = primal_value - dual_value
print("Duality gap:", gap)
```

Note that CVXPY does not directly return the $x$ of the dual problem; it merely provides $y$ and $s$. We recover the dual $x$ from the dual equality constraint. The pseudo-inverse (`pinv`) is needed if B is only positive semidefinite. As always, a near-zero duality gap indicates strong duality and certifies that the solution is optimal.

## Self-check questions

```{admonition} **Question**
:class: tip
Consider the least-squares problem in $\mathbb{R}^2$

$$
\begin{array}{ll}
\min_x & \| A x - a \|_2 \\
\text{subject to} &\\
& x_1 + x_2 = 1,\\
& x \ge 0
\end{array}
$$

with

$$
A = \begin{pmatrix}
2 & 1\\ 1 & 2
\end{pmatrix}, \qquad
a = \begin{pmatrix}
1\\ 2
\end{pmatrix}.
$$

Write down the primal and dual quadratic programme. What is the Lagrangian function?

You are provided with the certificates of optimality

$$
\begin{align*}
\bar{x}^{(1)} & = (0.5, 0.5)^\top,\\
(x^{(1)},y^{(1)},s^{(1)}) & = ((0.5, 0.5)^\top, 1, (0,0)^\top).
\end{align*}
$$

and

$$
\begin{align*}
\bar{x}^{(2)} & = (0, 1)^\top,\\
(x^{(2)},y^{(2)},s^{(2)}) & = ((0.5, 0.5)^\top, 0, (0,0)^\top).
\end{align*}
$$

Determine for each certificate whether it is a valid proof that $\bar{x}_i$ is optimal solution of the primal problem, $i = 1$ resp. $i = 2$.
```

```{dropdown} **Answer**
**Step 1. Primal quadratic programme:** First, we rewrite the objective $\|A x - a\|_2$ in a quadratic form:

$$
\|A x - a\|_2^2 
\;=\;
(A x - a)^\top (A x - a)
\;=\;
x^\top (A^\top A)\,x \;-\; 2\,a^\top A\,x \;+\; a^\top a,
$$

where $A^\top = A$ since $A$ is symmetric. Defining

$$
B := A^\top A 
\quad\text{and}\quad
b := -\,A^\top a,
$$

the quadratic objective takes the usual form $\tfrac12\,x^\top B x + b^\top x + \text{const}$. Concretely:

$$
B = A^2 = \begin{pmatrix}5 & 4\\ 4 & 5\end{pmatrix}, \qquad 
b = -\,A a = \begin{pmatrix}-4 \\ -5\end{pmatrix}.
$$

Ignoring the constant $a^\top a$ (which does not affect the minimiser), we get the primal quadratic programme:

$$
\begin{array}{ll}
\displaystyle \min_{x \in \mathbb{R}^2} 
& \tfrac12 \,x^\top B\,x \;+\; b^\top x 
\\[6pt]
\text{subject to} 
& x_1 + x_2 = 1,\\
& x_1 \ge 0,\; x_2 \ge 0.
\end{array}
$$


**Step 2. Dual quadratic programme:** We define the Lagrangian function with multipliers $y \in \mathbb{R}$ for the equality and $s \in \mathbb{R}^2_{\ge 0}$ for $x \ge 0$:

$$
L(x,y,s) 
\;=\;
\tfrac12\,x^\top B\,x 
\;+\;
b^\top x
\;-\;
y\,\bigl(x_1 + x_2 \,-\,1\bigr)
\;-\;
s^\top x.
$$

The term $-\,y\,(x_1+x_2 -1)$ encodes $x_1 + x_2 = 1$, while $-\,s^\top x$ encodes $x \ge 0$ with $s \ge 0$.

Reversing the order of $\min$ and $\max$ yields the dual:

$$
\begin{aligned}
\max_{(x,y,s), \; s \ge 0}
&&& y \;-\;\tfrac12\,x^\top B\,x
\\
\text{subject to}
&&& B\,x \;=\; y\,(1,1)^\top \;+\; s \;-\; (4,5)^\top,\quad s \ge 0.
\end{aligned}
$$

where $c^\top y = 1 \cdot y$ (because the equality constraint is $x_1+x_2=1$) and $d^\top s = 0$ (because $d = 0$ in the inequality constraints).

**Step 3. Lagrangian function:** From the above steps:

$$
L(x,y,s) \;=\; \tfrac12\,x^\top B\,x \;+\; b^\top x \;-\; y\,(x_1 + x_2 -1) \;-\; s^\top x,
$$

where $\;y \in \mathbb{R}$ and $\;s \in \mathbb{R}^2_{\ge 0}$.

**Step 4. Checking the first certificate of optimality:** We are given a *candidate* solution $\bar{x}^{(1)} = (0.5, 0.5)^\top$ and a *dual triple* $(\,x^{(1)},\,y^{(1)},\,s^{(1)}) = \bigl((0.5,0.5)^\top,\,1,\,(0,0)\bigr)$. We must check the *three* conditions for a valid certificate of optimality:

1. *Primal feasibility* of $\bar{x}$: This holds because $x_1^{(1)} + x_2^{(1)} = 0.5 + 0.5 = 1$ and $x_1^{(1)},\,x_2^{(1)} \ge 0$.  

2. *Dual feasibility* of $(x,y,s)$: This does *not* hold. While we have $s^{(1)} = (0,0) \ge 0$, the equality constraint is not satisfied:

$$
B\,x^{(1)} = \begin{pmatrix}4.5\\4.5\end{pmatrix} \neq y \begin{pmatrix}1\\1\end{pmatrix} + s - b = \begin{pmatrix}1\\1\end{pmatrix} + \begin{pmatrix}0\\0\end{pmatrix} - \begin{pmatrix}-4\\-5\end{pmatrix} = \begin{pmatrix}5\\6\end{pmatrix}.
$$

3. *Equality of primal and dual objective values* (only relevant once both are feasible).  

Because the second step *fails*, this certificate does *not* ensure that $\bar{x}^{(1)}$ is optimal. (However note that that the certificate fails does not prove that $\bar{x}^{(1)}$ is not optimal.)

**Step 5. Checking the second certificate of optimality:** We are given a *candidate* solution $\bar{x}^{(2)} = (0.5, 0.5)^\top$ and a *dual triple* $(\,x^{(2)},\,y^{(2)},\,s^{(2)}) = \bigl((0.5,0.5)^\top,\,1,\,(0,0)\bigr)$.

1. *Primal feasibility* of $\bar{x}$: This holds because $x_1 + x_2 = 0 + 1 = 1$ and $x_1,\,x_2 \ge 0$.  

2. *Dual feasibility* of $(x,y,s)$ This holds because $s = (0,0)$ satisfies $s \ge 0$ and

$$
B\,x = \begin{pmatrix}4\\5\end{pmatrix} = y \begin{pmatrix}1\\1\end{pmatrix} + s - b = \begin{pmatrix}0\\0\end{pmatrix} + \begin{pmatrix}0\\0\end{pmatrix} - \begin{pmatrix}-4\\-5\end{pmatrix}.
$$

3. *Equality of primal and dual objective values*: This holds because $\tfrac12 \,\bar{x}^\top B\,\bar{x} \;+\; b^\top \bar{x} = -2.5 = y \;-\;\tfrac12\,x^\top B\,x = -2.5$

Therefore certificate passed all checks and $\bar{x}^{(2)}$ is an optimal solution of the primal problem.
```

```{admonition} **Question**
:class: tip
The *Lagrangian function* associated with the primal problem is

$$
L(x, y, s) := \frac{1}{2} x^\top B x + b^\top x - y^\top (C x - c) - s^\top (D x - d).
$$

Show that

$$
\max_{y, s \atop s \geq 0} L(x, y, s) =
\begin{cases}
\frac{1}{2} x^\top B x + b^\top x & \text{if $C x = c$ and $D x \geq d$,}\\
+\infty & \text{otherwise.}
\end{cases}
$$
```

```{dropdown} **Answer**
If any component $(Cx - c)_i \neq 0$, we can send the corresponding $y_i$ to $+\infty$ or $-\infty$ (depending on the sign of $(Cx - c)_i$), causing the term $-\,y_i\,(Cx - c)_i$ to grow without bound.  Therefore, to keep the Lagrangian finite, we need $(Cx - c)_i = 0$ for all $i$, i.e. $Cx = c$.

Similarly, if $(Dx - d)_j < 0$ for some $j$, choosing $s_j \to +\infty$ makes $-\,s_j\,(Dx - d)_j$ go to $+\infty$. Hence, to avoid an unbounded Lagrangian, we require $Dx \ge d$. Thus, the maximum is finite only if $Cx = c$ and $Dx \ge d$. In that scenario, we can simply pick $y = 0$ and $s = 0$, yielding

$$
\max_{y,s \ge 0} L(x,y,s)
=
\tfrac{1}{2}\,x^\top B\,x + b^\top x.
$$

Hence,

$$
\max_{y, s \atop s \geq 0} L(x, y, s)
=
\begin{cases}
\tfrac{1}{2} x^\top B x + b^\top x & \text{if } C x = c \text{ and } D x \ge d,\\
+\infty & \text{otherwise.}
\end{cases}
$$
```

```{admonition} **Question**
:class: tip
Show that the dual problem

$$
\max_{y, s \atop s \geq 0} \min_x L(x, y, s) = \max_{y, s \atop s \geq 0} \min_x \Big[ \frac{1}{2} x^\top B x + b^\top x - y^\top (C x - c) - s^\top (D x - d) \Big]
$$

can be written as

$$
\begin{array}{ll}
\max_{x,y,s} & c^\top y + d^\top s - \frac{1}{2} x^\top B x\\
\text{subject to} &\\
& C^\top y + D^\top s - B x = b,\\
& s \ge 0.
\end{array}
$$
```

```{dropdown} **Answer**
We first examine the inner minimisation $\min_x L(x,y,s)$ for fixed $y$ and $s$. Differentiating $L$ with respect to $x$ and setting the gradient to zero gives

$$
0 = \nabla_{\!x} \, L (x,y,s) = Bx + b - C^\top y - D^\top s.
$$

If there are two solutions $x$, $\tilde{x}$ of $B x = C^\top y + D^\top s - b$ then 
  
$$
x^\top B\,x - \tilde{x}^\top B\,\tilde{x} = (x - \tilde{x})^\top (C^\top y + D^\top s - b) = (x - \tilde{x})^\top B\,x = 0
$$ 

because $x - \tilde{x} \in \ker B^\top = \ker B$. Therefore for any solution $\tilde{x}$ of $B x = C^\top y + D^\top s - b$, the inner minimisation problem satisfies

$$
\min_x L(x,y,s) \;=\; c^\top y \;+\; d^\top s \;-\; \tfrac12 \,\tilde{x}^\top B\,\tilde{x},
$$

using the simplification of $L$ arising from $B \tilde{x} = C^\top y + D^\top s - b$. If $B\,x = C^\top y + D^\top s - b$ has no solution then $x \mapsto L(x,y,s)$ has no stationary point and $\min_x L(x,y,s) = - \infty$.

We turn to the outer maximisation over all $y$ and $s\ge0$. If there are  $x$, $y$, $s$ such that $Bx = C^\top y + D^\top s - b$ then the maximum is attained among those. Otherwise the maximum is $- \infty$. Note that also $\max \emptyset = -\infty$. The dual problem therefore becomes:

$$
\begin{array}{ll}
\displaystyle \max_{x,y,s} 
& 
c^\top y + d^\top s \;-\; \tfrac12\,x^\top B\,x
\\[4pt]
\text{subject to} 
&
C^\top y + D^\top s - B\,x = b,
\quad
s \ge 0.
\end{array}
$$
```

```{admonition} **Question**
:class: tip
To examine optimisation problems without strong duality, we consider a possibly non-quadratic optimisation problem. Given $f : \mathbb{R}^n \to \mathbb{R}$, $h : \mathbb{R}^n \to \mathbb{R}^m$ and $g : \mathbb{R}^n \to \mathbb{R}^p$, let the Lagrangian function be

$$
L(x, y, s) :=f(x) - y^\top h(x) - s^\top g(x),
$$

giving rise to the primal problem

$$
p := \min_x \underbrace{\max_{y, s \atop s \geq 0} L(x, y, s)}_{=: P(x)} = \min_x P(x)
$$

and the dual problem

$$
d := \max_{y, s \atop s \geq 0} \underbrace{\min_x L(x, y, s)}_{=: D(y,s)} = \max_{y, s \atop s \geq 0} D(y,s).
$$

Suppose that a minimiser $\bar{x}$ of the primal problem exists and that a maximiser $(y,s)$ of the dual problem exists. For that $(y,s)$ let $x$ be a minimiser of $\min_x L(x, y, s)$. 

We say that strong duality does not hold if $d < p$; note that the inquality is strict. Explain why in this case $(\bar{x}, (x,y,s))$ cannot be used as certificate of optimimality as above.
```

````{dropdown} **Answer**
Strong duality provides an alternative way to identify optimal solutions: Given candidate solutions $\bar{x}$ and $(x, y, s)$, we plug them into the primal and dual problem, respectively. If they are feasible and return the same value, they are optimal. More formally, they are guaranteed to be optimal of primal and dual problem, respectively, if 

$$
L(\bar{x}, \bar{y}, \bar{s}) = L(x, y, s)
$$

where $\bar{y}$, $\bar{s}$ are maximisers of the inner primal optimisation problem $\max_{\bar{y}, \bar{s} \atop \bar{s} \geq 0} L(x, y, s)$.

But if strong duality does not hold, then this equality is not satisfied ($d \neq p$) and the alternative characterisation of optimal solutions fails.

**Note:** You may observe a slight asymmetry in the way we state the certificate of optimality: The primal problem $\min_x P(x) = \min_x \max_{y, s \atop s \geq 0} L(x, y, s)$ has the variable $x$ of the outer optimisation and $(y,s)$ of the inner optimisation. For the dual problem $\max_{y, s \atop s \geq 0} D(y,s) = \max_{y, s \atop s \geq 0} \min_x L(x, y, s)$ roles are reversed. The certificate $(\bar{x}, (x,y,s))$ refers to the outer variable of the *primal* problem while to the *inner and outer* variables of the dual problem. This happened because in the chapter above our starting point was the quadratic programme, in which $\bar{y}$, $\bar{s}$ do not appear explicitly. For the abstract theory we could always assume that $\bar{y}$, $\bar{s}$ can be recovered from solving the inner primal optimisation problem, restoring symmetry.
````

```{admonition} **Question**
:class: tip
You have three food products, labelled 1, 2, and 3, with the following characteristics:

| Product | Price per kg (pounds) | Protein content (g per kg) |
|---------|-----------------------|----------------------------|
| 1       | 5.0                  | 150                        |
| 2       | 3.5                  | 100                        |
| 3       | 7.0                  | 220                        |

You want to blend these products (in nonnegative quantities $x_1, x_2, x_3$ kg) to reach a target protein intake of $R = 200$ grams, but you also want to cap the cost at 6 pounds. However, it may not be possible to match the target exactly without violating the cost limit, so you decide to solve a least-squares problem: Minimise the deviation from 200 grams of protein per kg, i.e. 

   $$
   \min_{x} \;\|A x - a\|_2,
   $$

where $A = \begin{pmatrix} 150 & 100 & 220 \end{pmatrix}$, $a = 200$, subject to the cost constraint  $5\,x_1 + 3.5\,x_2 + 7\,x_3 \le 6$ and the nonnegativity constraint $x_1, x_2, x_3 \ge 0.$.

1. Rewrite the least-squares objective in the form 

   $$
   \frac12\,x^\top B\,x \;+\; b^\top x 
   \quad (\text{ignore any constant term}).
   $$

   Identify the explicit matrices $B$ and $b$.

2. Express all constraints in matrix form, i.e. $D x \ge d$, and state the final quadratic programme

   $$
   \min_{x} \quad \frac12\,x^\top B\,x + b^\top x
   \quad
   \text{subject to}
   \quad
   D x \ge d.
   $$

3. Interpret the role of the cost constraint within this least-squares setup. Under what circumstance might you exactly meet the 200-gram protein target?

4. Find the optimal solution with CVXPY.
```

````{dropdown} **Answer**
1. **Formulating the least-squares problem**:

   The (squared) least-squares objective is

   $$
   \|A x - a\|_2^2
   =
   \bigl(150\,x_1 + 100\,x_2 + 220\,x_3 - 200\bigr)^2.
   $$

   We can expand and then write this in the usual form $x^\top B\,x + b^\top x + \text{const}$. Specifically:

   $$
     B = A^\top A =
     \begin{pmatrix}
       150\\ 100\\ 220
     \end{pmatrix}
     \begin{pmatrix}
       150 & 100 & 220
     \end{pmatrix}
     =
     \begin{pmatrix}
       150^2 & 150\cdot 100 & 150\cdot 220\\
       100\cdot 150 & 100^2 & 100\cdot 220\\
       220\cdot 150 & 220\cdot 100 & 220^2
     \end{pmatrix}.
   $$

   and 

   $$
     b = - A^\top a = - \begin{pmatrix}
       200 \cdot 150\\ 200 \cdot 100\\ 200 \cdot 220
     \end{pmatrix}.
   $$

   Note that any constant term $200^2$ does not affect the minimiser.

2. **Constraints in matrix form**:

    We can write the cost constraint as

    $$
    \begin{pmatrix}
       5 & 3.5 & 7
    \end{pmatrix}
    \begin{pmatrix}
       x_1 \\ x_2 \\ x_3
    \end{pmatrix}
    \;\le\; 10.
    $$

    The non-negativity constraint is 

    $$
     \begin{pmatrix}
       1 & 0 & 0 \\
       0 & 1 & 0 \\
       0 & 0 & 1
     \end{pmatrix}
     x 
     \;\ge\;
     \begin{pmatrix}
       0 \\ 0 \\ 0
     \end{pmatrix}.
     $$

    Putting all inequalities together, we can define

    $$
    D 
    =
    \begin{pmatrix}
     -5 & -3.5 & -7 \\[4pt]
     1 & 0 & 0\\
     0 & 1 & 0\\
     0 & 0 & 1
    \end{pmatrix},
    \quad
    d
    =
    \begin{pmatrix}
     -10 \\ 0 \\ 0 \\ 0
    \end{pmatrix}.
    $$

    The final quadratic programme is

    $$
    \begin{aligned}
    \min_{x} \quad
    & \tfrac12 \,x^\top B\,x \;+\; b^\top x 
    \\
    \text{subject to}\quad
    & D\,x \;\ge\; d.
    \end{aligned}.
    $$

3. **Interpretation**:  
   - Minimising $\|A x - a\|_2$ steers $x$ toward achieving exactly 200 grams of protein.  
   - The constraint $\sum_i \text{price}_i\,x_i \le 6$ caps the total cost. If 200 grams of protein cannot be reached within 6 pounds, the solution will yield a mix $x$ that partially meets the target but does not exceed the cost.  
   - If the cost limit is large (say $\ge 10$), the solver will likely achieve $\|A x - a\|_2 = 0$ (i.e. exactly 200 grams) because there is no financial barrier to picking enough high-protein product.

---

4. **CVXPY:** We can solve the problem with CVXPY, using the below code. The optimal solution achieves approximately 189 grams per kile at a cost of 6 pounds. This code implements the least-squares problem in its original form. If a certificate of optimimality is needed, adapt the code of the Python skills section to solve this problem.

   ```python
   import cvxpy as cp
   import numpy as np
   
   # Problem data
   A = np.array([[150, 100, 220]])  # 1x3
   a = 200.0
   price = np.array([5.0, 3.5, 7.0])
   cost_limit = 6
   
   # Define variable
   x = cp.Variable(3)
   
   # Objective: least-squares
   # Minimize ||A x - a||_2^2 = (A x - a)^2 in this 1D case
   objective = cp.Minimize(cp.sum_squares(A @ x - a))
   
   # Constraints
   constraints = [
       price @ x <= cost_limit,  # cost constraint
       x >= 0                    # nonnegativity
   ]
   
   # Form and solve
   prob = cp.Problem(objective, constraints)
   result = prob.solve(solver=cp.SCS)
   
   print("Optimal objective (squared deviation):", result)
   print("Solution x:", x.value)
   print("Check cost:", price @ x.value, "(<= 10.0)")
   print("Protein intake:", A @ x.value, "(target 200)")
   ```
````
